{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6><b>\n",
    "    \n",
    "NYC Taxi Data download, subset and transfer to Postgres\n",
    "\n",
    "</b></font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "What's in this Notebook?\n",
    "    \n",
    "<p>\n",
    "\n",
    "</p>\n",
    "\n",
    "- **NYC Taxi data download and file format change**: through `gsutil` and bash \n",
    "\n",
    "<p>\n",
    "\n",
    "</p>\n",
    "\n",
    "- **Data brief inspection and column selection**: select columns for the particular use case\n",
    "<p>\n",
    "\n",
    "</p>\n",
    "\n",
    "- **Data read and transfer**: to the Postgres database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the NYC Taxi trip dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3> \n",
    "    \n",
    "Download the .zip file using [gsutil](https://cloud.google.com/storage/docs/gsutil) tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://hiring-test/data.zip...\n",
      "\\ [1 files][  1.4 GiB/  1.4 GiB]    3.2 MiB/s                                   \n",
      "Operation completed over 1 objects/1.4 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://hiring-test/data.zip ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3> \n",
    "    \n",
    "Unzip the file and change the file format of each corresponding file part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "  inflating: data/yellow_tripdata_2015-07_00  \n",
      "  inflating: data/yellow_tripdata_2015-01_00  \n",
      "  inflating: data/yellow_tripdata_2015-01_01  \n",
      "  inflating: data/yellow_tripdata_2015-01_02  \n",
      "  inflating: data/yellow_tripdata_2015-01_03  \n",
      "  inflating: data/yellow_tripdata_2015-01_04  \n",
      "  inflating: data/yellow_tripdata_2015-01_05  \n",
      "  inflating: data/yellow_tripdata_2015-01_06  \n",
      "  inflating: data/yellow_tripdata_2015-01_07  \n",
      "  inflating: data/yellow_tripdata_2015-01_08  \n",
      "  inflating: data/yellow_tripdata_2015-01_09  \n",
      "  inflating: data/yellow_tripdata_2015-01_10  \n",
      "  inflating: data/yellow_tripdata_2015-01_11  \n",
      "  inflating: data/yellow_tripdata_2015-01_12  \n",
      "  inflating: data/yellow_tripdata_2015-01_13  \n",
      "  inflating: data/yellow_tripdata_2015-01_14  \n",
      "  inflating: data/yellow_tripdata_2015-01_15  \n",
      "  inflating: data/yellow_tripdata_2015-01_16  \n",
      "  inflating: data/yellow_tripdata_2015-01_17  \n",
      "  inflating: data/yellow_tripdata_2015-01_18  \n",
      "  inflating: data/yellow_tripdata_2015-01_19  \n",
      "  inflating: data/yellow_tripdata_2015-01_20  \n",
      "  inflating: data/yellow_tripdata_2015-01_21  \n",
      "  inflating: data/yellow_tripdata_2015-01_22  \n",
      "  inflating: data/yellow_tripdata_2015-01_23  \n",
      "  inflating: data/yellow_tripdata_2015-01_24  \n",
      "  inflating: data/yellow_tripdata_2015-01_25  \n",
      "  inflating: data/yellow_tripdata_2015-04_00  \n",
      "  inflating: data/yellow_tripdata_2015-04_01  \n",
      "  inflating: data/yellow_tripdata_2015-04_02  \n",
      "  inflating: data/yellow_tripdata_2015-04_03  \n",
      "  inflating: data/yellow_tripdata_2015-04_04  \n",
      "  inflating: data/yellow_tripdata_2015-04_05  \n",
      "  inflating: data/yellow_tripdata_2015-04_06  \n",
      "  inflating: data/yellow_tripdata_2015-04_07  \n",
      "  inflating: data/yellow_tripdata_2015-04_08  \n",
      "  inflating: data/yellow_tripdata_2015-04_09  \n",
      "  inflating: data/yellow_tripdata_2015-04_10  \n",
      "  inflating: data/yellow_tripdata_2015-04_11  \n",
      "  inflating: data/yellow_tripdata_2015-04_12  \n",
      "  inflating: data/yellow_tripdata_2015-04_13  \n",
      "  inflating: data/yellow_tripdata_2015-04_14  \n",
      "  inflating: data/yellow_tripdata_2015-04_15  \n",
      "  inflating: data/yellow_tripdata_2015-04_16  \n",
      "  inflating: data/yellow_tripdata_2015-04_17  \n",
      "  inflating: data/yellow_tripdata_2015-04_18  \n",
      "  inflating: data/yellow_tripdata_2015-04_19  \n",
      "  inflating: data/yellow_tripdata_2015-04_20  \n",
      "  inflating: data/yellow_tripdata_2015-04_21  \n",
      "  inflating: data/yellow_tripdata_2015-04_22  \n",
      "  inflating: data/yellow_tripdata_2015-04_23  \n",
      "  inflating: data/yellow_tripdata_2015-04_24  \n",
      "  inflating: data/yellow_tripdata_2015-04_25  \n",
      "  inflating: data/yellow_tripdata_2015-04_26  \n",
      "  inflating: data/yellow_tripdata_2015-07_01  \n",
      "  inflating: data/yellow_tripdata_2015-07_02  \n",
      "  inflating: data/yellow_tripdata_2015-07_03  \n",
      "  inflating: data/yellow_tripdata_2015-07_04  \n",
      "  inflating: data/yellow_tripdata_2015-07_05  \n",
      "  inflating: data/yellow_tripdata_2015-07_06  \n",
      "  inflating: data/yellow_tripdata_2015-07_07  \n",
      "  inflating: data/yellow_tripdata_2015-07_08  \n",
      "  inflating: data/yellow_tripdata_2015-07_09  \n",
      "  inflating: data/yellow_tripdata_2015-07_10  \n",
      "  inflating: data/yellow_tripdata_2015-07_11  \n",
      "  inflating: data/yellow_tripdata_2015-07_12  \n",
      "  inflating: data/yellow_tripdata_2015-07_13  \n",
      "  inflating: data/yellow_tripdata_2015-07_14  \n",
      "  inflating: data/yellow_tripdata_2015-07_15  \n",
      "  inflating: data/yellow_tripdata_2015-07_16  \n",
      "  inflating: data/yellow_tripdata_2015-07_17  \n",
      "  inflating: data/yellow_tripdata_2015-07_18  \n",
      "  inflating: data/yellow_tripdata_2015-07_19  \n",
      "  inflating: data/yellow_tripdata_2015-07_20  \n",
      "  inflating: data/yellow_tripdata_2015-07_21  \n",
      "  inflating: data/yellow_tripdata_2015-07_22  \n",
      "  inflating: data/yellow_tripdata_2015-07_23  \n"
     ]
    }
   ],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each file in the data directory, change it's name and save them in .csv format\n",
    "\n",
    "!for f in data/*; do mv -- \"$f\" \"${f%}.csv\"; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data brief inspection and column selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import cartoframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect first file with header to get familiarized with the data\n",
    "\n",
    "df = pd.read_csv('data/yellow_tripdata_2015-01_00.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>RateCodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-15 19:05:39</td>\n",
       "      <td>2015-01-15 19:23:42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.59</td>\n",
       "      <td>-73.993896</td>\n",
       "      <td>40.750111</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.974785</td>\n",
       "      <td>40.750618</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:38</td>\n",
       "      <td>2015-01-10 20:53:28</td>\n",
       "      <td>1</td>\n",
       "      <td>3.30</td>\n",
       "      <td>-74.001648</td>\n",
       "      <td>40.724243</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.994415</td>\n",
       "      <td>40.759109</td>\n",
       "      <td>1</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:38</td>\n",
       "      <td>2015-01-10 20:43:41</td>\n",
       "      <td>1</td>\n",
       "      <td>1.80</td>\n",
       "      <td>-73.963341</td>\n",
       "      <td>40.802788</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.951820</td>\n",
       "      <td>40.824413</td>\n",
       "      <td>2</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:39</td>\n",
       "      <td>2015-01-10 20:35:31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-74.009087</td>\n",
       "      <td>40.713818</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-74.004326</td>\n",
       "      <td>40.719986</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:39</td>\n",
       "      <td>2015-01-10 20:52:58</td>\n",
       "      <td>1</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-73.971176</td>\n",
       "      <td>40.762428</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-74.004181</td>\n",
       "      <td>40.742653</td>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         2  2015-01-15 19:05:39   2015-01-15 19:23:42                1   \n",
       "1         1  2015-01-10 20:33:38   2015-01-10 20:53:28                1   \n",
       "2         1  2015-01-10 20:33:38   2015-01-10 20:43:41                1   \n",
       "3         1  2015-01-10 20:33:39   2015-01-10 20:35:31                1   \n",
       "4         1  2015-01-10 20:33:39   2015-01-10 20:52:58                1   \n",
       "\n",
       "   trip_distance  pickup_longitude  pickup_latitude  RateCodeID  \\\n",
       "0           1.59        -73.993896        40.750111           1   \n",
       "1           3.30        -74.001648        40.724243           1   \n",
       "2           1.80        -73.963341        40.802788           1   \n",
       "3           0.50        -74.009087        40.713818           1   \n",
       "4           3.00        -73.971176        40.762428           1   \n",
       "\n",
       "  store_and_fwd_flag  dropoff_longitude  dropoff_latitude  payment_type  \\\n",
       "0                  N         -73.974785         40.750618             1   \n",
       "1                  N         -73.994415         40.759109             1   \n",
       "2                  N         -73.951820         40.824413             2   \n",
       "3                  N         -74.004326         40.719986             2   \n",
       "4                  N         -74.004181         40.742653             2   \n",
       "\n",
       "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0         12.0    1.0      0.5        3.25           0.0   \n",
       "1         14.5    0.5      0.5        2.00           0.0   \n",
       "2          9.5    0.5      0.5        0.00           0.0   \n",
       "3          3.5    0.5      0.5        0.00           0.0   \n",
       "4         15.0    0.5      0.5        0.00           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  \n",
       "0                    0.3         17.05  \n",
       "1                    0.3         17.80  \n",
       "2                    0.3         10.80  \n",
       "3                    0.3          4.80  \n",
       "4                    0.3         16.30  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
       "       'passenger_count', 'trip_distance', 'pickup_longitude',\n",
       "       'pickup_latitude', 'RateCodeID', 'store_and_fwd_flag',\n",
       "       'dropoff_longitude', 'dropoff_latitude', 'payment_type', 'fare_amount',\n",
       "       'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n",
       "       'improvement_surcharge', 'total_amount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Given that our target is to understand how demographic and socio-economic factors drive the number of pickups by block group in NYC, we'll keep only the **pickup date** (to compute average per day) and **pickup location** (longitude and latitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data read and transfer process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "The ETL process will consist on the following steps,\n",
    "    \n",
    "<p></p><p></p><p></p>\n",
    "    \n",
    "- Creating a table in the Postgres database with the corresponding data types\n",
    "    \n",
    "<p></p><p></p><p></p>\n",
    "\n",
    "- Subsetting the data, keeping the desired columns\n",
    "    \n",
    "<p></p><p></p><p></p>\n",
    "\n",
    "- Cleaning it by removing the header lines\n",
    "\n",
    "<p></p><p></p><p></p>    \n",
    "\n",
    ">**NOTE:** Geospatial out-of-bounds records are not filtered at this point as these would be discarded later when performing geospatial operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "For this purpose, a combination of Python packages, Unix tools and [Miller](https://miller.readthedocs.io/) will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for database connection\n",
    "\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SQLAlchemy engine to connect to the database using pandas built-in functions\n",
    "\n",
    "engine = create_engine('postgresql+psycopg2://postgres:postgres@postgres/postgres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the dataframe, using only the desired columns and 0 records to create a table with the data structure in pg\n",
    "\n",
    "subset_df = df[['tpep_pickup_datetime', 'pickup_longitude', 'pickup_latitude']][:0]\n",
    "\n",
    "# Change if_exists to 'replace' if you'd like to overwrite the table \n",
    "\n",
    "subset_df.to_sql(name='nyc_taxi_sub_second', schema='mmoncada', con=engine, index=False, if_exists='fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tpep_pickup_datetime, pickup_longitude, pickup_latitude]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the operation performed successfully\n",
    "\n",
    "df_test = pd.read_sql(sql=\"\"\"SELECT * FROM mmoncada.nyc_taxi_sub_second\"\"\", con=engine)\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the header in another CSV file (only if needed anytime)\n",
    "\n",
    "!head -n 1 data/yellow_tripdata_2015-01_00.csv > header.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the header from each first file (files ending in _00 start with header)\n",
    "\n",
    "!sed -i 1d data/*_00.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "As an alternative to the steps above, the following miller command could be executed. \n",
    "    \n",
    "<p></p><p></p><p></p>\n",
    "    \n",
    "Furthermore, it'd be possible to use this command in combination with `cat` (or similar) to avoid hard-coding the header corresponding value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any header row values -- Alternative (probably better) to the above\n",
    "\n",
    "#!mlr --csv --implicit-csv-header --headerless-csv-output filter -x '$1==\"tpep_pickup_datetime\"' complete_yellow_data.csv > complete_yellow_data_clean.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the undesired columns (keep tpep_pickup_datetime, pickup_longitude, pickup_latitude) and append to a csv file\n",
    "!mlr --csv --implicit-csv-header --headerless-csv-output cut -f 2,6,7 data/yellow*.csv >> complete_yellow_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check there are no lines containing tpep_pickup_datetime (as header lines would)\n",
    "\n",
    "!cat complete_yellow_data.csv | grep -in tpep_pickup_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "Once we've subset the data and confirmed that there are no headers within, we'll use PostgreSQL [COPY](https://www.postgresql.org/docs/12/sql-copy.html) from command for data transfer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters to connect to the PostgreSQL database\n",
    "\n",
    "param_dict = {\n",
    "    \"host\": \"postgres\",\n",
    "    \"database\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to PostgreSQL database using psycopg2\n",
    "\n",
    "con = psycopg2.connect(**param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_from_file(con, file_path, table):\n",
    "    \"\"\"\n",
    "    Function that uses PostgreSQL COPY from to transfer data from a CSV file to the database\n",
    "    args:\n",
    "        con: connection to database (psycopg2 connection)\n",
    "        file_path: file path for the target CSV file\n",
    "        table: schema and table name in the format schema.table_name (str)\n",
    "    \"\"\"\n",
    "    cursor = con.cursor()\n",
    "    file = open(file_path, 'r')\n",
    "    try:\n",
    "        cursor.copy_from(file, table, sep=\",\")\n",
    "        con.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(f\"Some error has ocurred: {error}\")\n",
    "        con.rollback()\n",
    "        cursor.close()\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.4 s, sys: 7.74 s, total: 20.1 s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_from_file(con=con, file_path='complete_yellow_data.csv', table='mmoncada.nyc_taxi_sub_second')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the resulting table in Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "After creating the table, a new column has been created as the table's primary key\n",
    "    \n",
    "<p></p><p></p><p></p>\n",
    "    \n",
    "<font size=3>\n",
    "\n",
    "Afterwards, a geometry column has been created in the `nyc_taxi_sub` table and the contents have been filled using PostGIS [`ST_Point`](https://postgis.net/docs/ST_Point.html) function, in addition, a spatial index has been added to speed up geospatial operations used later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "ALTER TABLE nyc_taxi_sub ADD COLUMN id SERIAL PRIMARY KEY;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT AddGeometryColumn('mmoncada','nyc_taxi_sub','the_geom',4326,'POINT',2);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "UPDATE nyc_taxi_sub SET the_geom = ST_SetSRID(ST_Point(pickup_longitude, pickup_latitude), 4326);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "A new column, `date_pickup` has been created and populated with the first 10 characters of `tpep_pickup_datetime`\n",
    "    \n",
    "<p></p><p></p><p></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "ALTER TABLE nyc_taxi_sub ADD COLUMN date_pickup VARCHAR;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "UPDATE nyc_taxi_sub SET date_pickup = LEFT(tpep_pickup_datetime, 10);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging data based on spatial relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "CREATE TABLE taxi_block AS (\n",
    "\n",
    "\tSELECT \n",
    "\t\tCOUNT(taxi.id) as number_pickups, \n",
    "\t\tblock.geoid, \n",
    "\t\tblock.do_label \n",
    "\tFROM \n",
    "\t\tnyc_taxi_sub taxi, \n",
    "\t\tdo_sync_usa_acs_demographics_sociodemographics_usa_blockgroup_2 block \n",
    "\tWHERE \n",
    "\t\tST_Intersects(taxi.the_geom, block.the_geom) \n",
    "\tGROUP BY block.geoid, block.do_label\n",
    "); \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>\n",
    "\n",
    "\n",
    "Access the next Notebook, [`3 - ESDA data merged`](3%20-%20ESDA%20data%20merged.ipynb)\n",
    "\n",
    "</font> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
