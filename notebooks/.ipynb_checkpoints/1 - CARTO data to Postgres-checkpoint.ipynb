{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6><b>\n",
    "    \n",
    "ACS Data download and transfer CARTO - Postgres\n",
    "\n",
    "</b></font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "What's in this Notebook?\n",
    "\n",
    "<p>\n",
    "\n",
    "</p>\n",
    "\n",
    "- **ACS Demographics data download and transfer**: from CARTO's Data observatory to CARTO and Python SDK\n",
    "<p>\n",
    "\n",
    "</p>\n",
    "\n",
    "- **NYC boundaries data download and transfer**: from `data.cityofnewyork.us` through GeoPandas\n",
    "    \n",
    "<p>\n",
    "\n",
    "</p>\n",
    "\n",
    "- **Join do_label information**: from `block_group_label` and add it to ACS Sociodemographics dataset through SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    ">**NOTE:** given that the data within the provided bucket wasn't public, a different data source with (apparently) the same information has been used\n",
    "\n",
    "Datasets used: \n",
    "\n",
    "- https://carto.com/spatial-data-catalog/browser/dataset/acs_sociodemogr_2396d534/ \n",
    "    \n",
    "<p>\n",
    "\n",
    "</p>\n",
    "\n",
    "- https://carto.com/spatial-data-catalog/browser/geography/cdb_blockgroup_7753dd51/\n",
    "    \n",
    "<p>\n",
    "\n",
    "</p>\n",
    "\n",
    "- https://data.cityofnewyork.us/api/geospatial/tqmj-j8zm?method=export&format=Shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "><b>NOTE:</b> This Notebook purpose is to show the followed process to load the referred datasets into the recreated database\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download and transfer process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import cartoframes\n",
    "import psycopg2\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from carto.auth import APIKeyAuthClient\n",
    "from carto.sql import SQLClient, CopySQLClient\n",
    "from cartoframes.auth import Credentials\n",
    "from cartoframes.utils import decode_geometry\n",
    "from shapely.geometry.base import BaseGeometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Context Manager to open the json file containing the CARTO account credentials\n",
    "\n",
    "with open(\"carto_creds.json\") as config:\n",
    "    config = json.load(config)\n",
    "    \n",
    "username = config.get('username')\n",
    "api_key = config.get('api_key')\n",
    "schema = 'mmoncada'\n",
    "if_exists = 'replace'\n",
    "\n",
    "table_list = ['block_group_label', 'do_sync_usa_acs_demographics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database connection parameters\n",
    "\n",
    "param_dict = {\n",
    "    \"host\": \"postgres\",\n",
    "    \"database\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_carto_dataset(username, api_key, table_name): \n",
    "    \"\"\"\n",
    "    Function to download a CARTO dataset as a CSV file using COPY to command through CARTO's SQL API\n",
    "    \n",
    "    Returns name of the downloaded file (file_name, str)\n",
    "    args:\n",
    "        username: CARTO account username (str)\n",
    "        api_key: CARTO API key with access to the dataset (str)\n",
    "        table_name: Target CARTO dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    file_name = f\"{table_name}.csv\"\n",
    "    \n",
    "    print(f\"Downloading dataset {table_name}\")\n",
    "\n",
    "    base_url = f\"https://{username}.carto.com\"\n",
    "\n",
    "    copy_client = CopySQLClient(APIKeyAuthClient(base_url, api_key))\n",
    "\n",
    "    to_query = f\"COPY {table_name} TO stdout WITH (FORMAT csv, HEADER true)\"\n",
    "\n",
    "    copy_client.copyto_file_path(to_query, f'{table_name}.csv')\n",
    "\n",
    "    print(f\"Dataset {table_name} downloaded\")\n",
    "        \n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_database(host, database, user, password):\n",
    "    \"\"\"\n",
    "    Function to connect to a PostgreSQL database \n",
    "    \n",
    "    Returns SQLAlchemy and psycopg2 connection objects\n",
    "    args:\n",
    "        host: database server corresponding host (str)\n",
    "        database: database name (str)\n",
    "        user: database target user (str)\n",
    "        password: user's corresponding password (str)\n",
    "    \"\"\"\n",
    "    \n",
    "    engine = create_engine(f\"postgresql+psycopg2://{host}:{user}@{password}/{database}\")\n",
    "    \n",
    "    con = psycopg2.connect(host=host, database=database, user=user, password=password)\n",
    "    \n",
    "    return engine, con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_table_name_length(table_name):\n",
    "    \"\"\"\n",
    "    Function to check if a table name length is below PostgreSQL 63 byte limit\n",
    "    \n",
    "    Returns table name below this limit, truncating original name if necessary (table_name, str)\n",
    "    args:\n",
    "        host: database server corresponding host (str)\n",
    "        database: database name (str)\n",
    "        user: database target user (str)\n",
    "        password: user's corresponding password (str)\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(table_name) >= 63:\n",
    "        \n",
    "        table_name = table_name[:62]\n",
    "        \n",
    "        print(f\"Table name too large, truncating to {table_name}\")\n",
    "        \n",
    "    return table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_postgis(file_name, table_name, schema, engine, con, if_exists='replace'):\n",
    "    \"\"\"\n",
    "    Function that given a CSV file path, creates a table inside the Postgres database \n",
    "    with the correct data structure\n",
    "    \n",
    "    Depends on function check_table_length\n",
    "    \n",
    "    This function reads the CSV file, creates a GeoDataFrame, formats the data to upload to PostgreSQL\n",
    "    and creates a table with the desired data types and column order.\n",
    "    \n",
    "    Returns value to indicate if COPY process should happen (proceed_copy, bool) \n",
    "    and psycopg2 cursor object (cursor)\n",
    "    args:\n",
    "        file_name: path to csv file (str)\n",
    "        table_name: desired database table name (str)\n",
    "        schema: target database schema (str)\n",
    "        host: connection host (str)\n",
    "        database: target database (str)\n",
    "        user: connection user (str)\n",
    "        password: password for user (str)\n",
    "        if_exists: defines how to behave if the table already exists {'fail', 'replace'}, default 'replace'\n",
    "                   - fail: Raise a ValueError\n",
    "                   - replace: Drop the table before inserting new values\n",
    "    \"\"\"\n",
    "    \n",
    "    COLLISION_STRATEGIES = ['fail', 'replace']\n",
    "    \n",
    "    if if_exists not in COLLISION_STRATEGIES:\n",
    "        print(\"if_exists was not in available options, please try 'fail' or 'replace'\")\n",
    "        raise ValueError\n",
    "    \n",
    "    proceed_copy = True\n",
    "    \n",
    "    cursor = con.cursor()\n",
    "    \n",
    "    value = BaseGeometry()\n",
    "    \n",
    "    table_name = check_table_name_length(table_name)\n",
    "\n",
    "    df = pd.read_csv(file_name, nrows=10)\n",
    "    \n",
    "    columns_ordered = [col if (col != 'the_geom') else 'geometry' for col in df.columns.values]\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(df, crs='EPSG:4326', geometry=decode_geometry(df['the_geom']))\n",
    "\n",
    "    gdf['geometry'].fillna(value, inplace=True)\n",
    "\n",
    "    gdf.drop('the_geom', axis=1, inplace=True)\n",
    "\n",
    "    gdf = gdf.reindex(columns=columns_ordered)\n",
    "    \n",
    "    try:\n",
    "\n",
    "        gdf.astype(object).to_postgis(name=table_name, schema=schema, con=engine, if_exists=if_exists)\n",
    "        \n",
    "        cursor.execute(f\"truncate table {schema}.{table_name};\")\n",
    "\n",
    "        cursor.execute(f\"alter table {schema}.{table_name} rename column geometry to the_geom;\")\n",
    "\n",
    "    except Exception as e:\n",
    "        proceed_copy = False\n",
    "        print(f\"Some error ocurred creating table {e}\")\n",
    "    \n",
    "    return proceed_copy, cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_postgis(file_name, table_name, schema, host, database, user, password, if_exists='replace'):\n",
    "    \"\"\"\n",
    "    Function that uploads a dataset (CSV file) to a Postgres database.\n",
    "    \n",
    "    Depends on function create_table_postgis\n",
    "    Once the table is created, it uses PostgreSQL COPY from command to upload the data\n",
    "    args:\n",
    "        file_name: path to csv file (str)\n",
    "        table_name: desired database table name (str)\n",
    "        schema: target database schema (str)\n",
    "        host: connection host (str)\n",
    "        database: target database (str)\n",
    "        user: connection user (str)\n",
    "        password: password for user (str)\n",
    "    \"\"\"\n",
    "    \n",
    "    engine, con = connect_database(host=host, database=database, \n",
    "                                   user=user, password=password)\n",
    "    \n",
    "    proceed_copy, cursor = create_table_postgis(file_name=file_name, table_name=table_name, \n",
    "                                                schema=schema, engine=engine, con=con, if_exists=if_exists)\n",
    "    \n",
    "    if proceed_copy:\n",
    "        \n",
    "        copy_sql = f\"\"\"\n",
    "               COPY {schema}.{table_name} FROM stdin WITH CSV HEADER\n",
    "               DELIMITER as ','\n",
    "               \"\"\"\n",
    "        print(f\"Copying dataset {table_name} to postgres\")\n",
    "    \n",
    "        with open(file_name, 'r') as f:\n",
    "\n",
    "            try:\n",
    "                cursor.copy_expert(sql=copy_sql, file=f)\n",
    "                con.commit()\n",
    "                print(f\"Dataset {table_name} copied to postgres\")\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(f\"Some error ocurred copying dataset {error}\")\n",
    "                con.rollback()\n",
    "            cursor.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carto_to_postgis(username, table_name, api_key, schema, host, database, user, password, if_exists='replace'):\n",
    "    \"\"\"\n",
    "    Function that downloads a CARTO dataset and uploads it to a PostgreSQL database.\n",
    "    \n",
    "    Depends on download_carto_dataset and dataset_to_postgis.\n",
    "    args:\n",
    "        username: CARTO account username (str)\n",
    "        api_key: CARTO API key with access to the dataset (str)\n",
    "        table_name: desired database table name (str)\n",
    "        schema: target database schema (str)\n",
    "        host: connection host (str)\n",
    "        database: target database (str)\n",
    "        user: connection user (str)\n",
    "        password: password for user (str)\n",
    "    \"\"\"\n",
    "    \n",
    "    file_name = download_carto_dataset(username=username, api_key=api_key, \n",
    "                                       table_name=table_name)\n",
    "    \n",
    "    dataset_to_postgis(file_name=file_name, table_name=table_name, schema=schema, host=host, \n",
    "                       database=database, user=user, password=password, if_exists=if_exists)\n",
    "    \n",
    "    os.remove(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset block_group_label\n",
      "Dataset block_group_label downloaded\n",
      "Copying dataset block_group_label to postgres\n",
      "Dataset block_group_label copied to postgres\n",
      "Downloading dataset do_sync_usa_acs_demographics\n",
      "Dataset do_sync_usa_acs_demographics downloaded\n",
      "Copying dataset do_sync_usa_acs_demographics to postgres\n",
      "Dataset do_sync_usa_acs_demographics copied to postgres\n"
     ]
    }
   ],
   "source": [
    "for table_name in table_list:\n",
    "\n",
    "    carto_to_postgis(username=username, table_name=table_name, api_key=api_key, \n",
    "                     schema=schema, **param_dict, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch NYC boundaries and transfer the data to PostgreSQL + PostGIS database\n",
    "\n",
    "engine, _ = connect_database(host='postgres', database='postgres', \n",
    "                             user='postgres', password='postgres')\n",
    "\n",
    "nyc_gdf = gpd.read_file(\n",
    "    'https://data.cityofnewyork.us/api/geospatial/tqmj-j8zm?method=export&format=Shapefile')\n",
    "\n",
    "nyc_gdf.to_postgis(name='nyc_boundaries', con=engine, schema=schema, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge information from ACS tables  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "A new column has been created in the `ACS Sociodemographics` dataset and the contents have been filled using the `block_group_label` _do_label_ column based on the `geoid` column matching from both datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "ALTER TABLE do_sync_usa_acs_demographics_sociodemographics_usa_blockgroup_2 ADD COLUMN do_label VARCHAR;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "UPDATE do_sync_usa_acs_demographics_sociodemographics_usa_blockgroup_2 as census SET do_label = labels.do_label FROM block_group_label as labels WHERE census.geoid = labels.geoid;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>\n",
    "\n",
    "\n",
    "Access the next Notebook, [`2 - Data cleaning and data transfer`](2%20-%20Data%20cleaning%20and%20data%20transfer.ipynb)\n",
    "\n",
    "</font> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
